{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LR4bovYL4CJz"
   },
   "source": [
    "## COMP5625M Assessment 2 - Image Caption Generation [100 marks]\n",
    "\n",
    "<div class=\"logos\"><img src=\"./Comp5625M_logo.jpg\" width=\"220px\" align=\"right\"></div>\n",
    "\n",
    "The maximum marks for each part are shown in the section headers. The overall assessment carries a total of 100 marks.\n",
    "\n",
    "This assessment is weighted 25% of the final grade for the module.\n",
    "\n",
    "### Motivation \n",
    "\n",
    "Through this assessment, you will:\n",
    "\n",
    "> 1. Understand the principles of text pre-processing and vocabulary building.\n",
    "> 2. Gain experience working with an image-to-text model.\n",
    "> 3. Use and compare two text similarity metrics for evaluating an image-to-text model, and understand evaluation challenges.\n",
    "\n",
    "\n",
    "### Setup and resources \n",
    "\n",
    "Having a GPU will speed up the image feature extraction process. If you want to use a GPU, please refer to the module website for recommended working environments with GPUs.\n",
    "\n",
    "Please implement the coursework using PyTorch and Python-based libraries, and refer to the notebooks and exercises provided.\n",
    "\n",
    "This assessment will use a subset of the [COCO \"Common Objects in Context\" dataset](https://cocodataset.org/) for image caption generation. COCO contains 330K images of 80 object categories, and at least five textual reference captions per image. Our subset consists of nearly 5070 of these images, each with five or more different descriptions of the salient entities and activities, and we will refer to it as COCO_5070.\n",
    "\n",
    "To download the data:\n",
    "\n",
    "> 1. **Images and annotations**: download the zipped file provided in the link here as [``COMP5625M_data_assessment_2.zip``](https://leeds365-my.sharepoint.com/:u:/g/personal/scssali_leeds_ac_uk/EWWzE-_AIrlOkvOKxH4rjIgBF_eUx8KDJMPKM2eHwCE0dg?e=DdX62H). \n",
    "\n",
    "``Info only:`` To understand more about the COCO dataset, you can look at the [download page](https://cocodataset.org/#download). We have already provided you with the \"2017 Train/Val annotations (241MB)\", but our image subset consists of fewer images than the original COCO dataset. **So, no need to download anything from here!** \n",
    "\n",
    "> 2. **Image metadata**: as our set is a subset of the full COCO dataset, we have created a CSV file containing relevant metadata for our particular subset of images. You can also download it from Drive, \"coco_subset_meta.csv\", at the same link as 1.\n",
    "\n",
    "\n",
    "### Submission\n",
    "\n",
    "Please submit the following:\n",
    "\n",
    "> 1. Your completed Jupyter notebook file, in .ipynb format. **Do not change the file name.**\n",
    "> 2. The .html version of your notebook; File > Download as > HTML (.html). Check that all cells have been run and all outputs (including all graphs you would like to be marked) are displayed in the .html for marking.\n",
    "\n",
    "**Final note:**\n",
    "\n",
    "> **Please include everything you would like to be marked in this notebook, including figures. Under each section, put the relevant code containing your solution. You may re-use functions you defined previously, but any new code must be in the appropriate section.** Feel free to add as many code cells as you need under each section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your student username (for example, ```sc15jb```):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your full name:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Feel free to add to this section as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\ananconda\\\\envs\\\\torch\\\\python.exe'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the python version\n",
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze >requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect which device (CPU/GPU) to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic principle of our image-to-text model is as pictured in the diagram below, where an Encoder network encodes the input image as a feature vector by providing the outputs of the last convolutional layer of a pre-trained CNN (we use [ResNet50](https://arxiv.org/abs/1512.03385)). This pretrained network has been trained on the complete ImageNet dataset and is thus able to recognise common objects. \n",
    "\n",
    "**(Hint)** You can alternatively use the COCO trained pretrained weights from [PyTorch](https://pytorch.org/vision/stable/models.html). One way to do this is use the \"FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1\" but use e.g., \"resnet_model = model.backbone.body\". Alternatively, you can use the checkpoint from your previous coursework where you finetuned to COCO dataset. \n",
    "\n",
    "These features are then fed into a Decoder network along with the reference captions. As the image feature dimensions are large and sparse, the Decoder network includes a linear layer which downsizes them, followed by *a batch normalisation layer* to speed up training. Those resulting features, as well as the reference text captions, are then passed into a recurrent network (we will use **RNN** in this assessment). \n",
    "\n",
    "The reference captions used to compute loss are represented as numerical vectors via an **embedding layer** whose weights are learned during training.\n",
    "\n",
    "<!-- ![Encoder Decoder](comp5625M_figure.jpg) --> \n",
    "\n",
    "\n",
    "<div>\n",
    "<center><img src=\"comp5625M_figure_imageCaption.jpg\" width=\"1000\"/></center>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder-Decoder network could be coupled and trained end-to-end, without saving features to disk; however, this requires iterating through the entire image training set during training. We can make the **training more efficient by decoupling the networks**. Thus, we will:\n",
    "\n",
    "> First extract the feature representations of the images from the Encoder\n",
    "\n",
    "> Save these features (Part 1) such that during the training of the Decoder (Part 3), we only need to iterate over the image feature data and the reference captions.\n",
    "\n",
    "**Hint**\n",
    "Try commenting out the feature extraction part once you have saved the embeddings. This way if you have to re-run the entire codes for some reason then you can only load these features. \n",
    "\n",
    "\n",
    "### Overview\n",
    "\n",
    "> 1. Extracting image features \n",
    "> 2. Text preparation of training and validation data \n",
    "> 3. Training the decoder\n",
    "> 4. Generating predictions on test data\n",
    "> 5. Caption evaluation via BLEU score\n",
    "> 6. Caption evaluation via Cosine similarity\n",
    "> 7. Comparing BLEU and Cosine similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Extracting image features [11 marks]\n",
    "\n",
    "> 1.1 Design a encoder layer with pretrained ResNet50 (4 marks)\n",
    "\n",
    "> 1.2 Image feature extraction step (7 marks)\n",
    "\n",
    "#### 1.1 Design a encoder layer with pretrained ResNet50 (4 marks)\n",
    "\n",
    "> Read through the template EncoderCNN class below and complete the class.\n",
    "\n",
    "> You are expected to use ResNet50 pretrained on imageNet provided in the Pytorch library (torchvision.models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Load the pretrained ResNet-50 and replace top fc layer.\"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        # Your code here!\n",
    "\n",
    "        # TO COMPLETE\n",
    "        # keep all layers of the pretrained net except the last layers of fully-connected ones (you are permitted to take other layers too but this can affect your accuracy!)\n",
    "\n",
    "        # Load the pretrained ResNet-50 model\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # Remove the last (classification) layer\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        \n",
    "        # Create the modified ResNet-50 model\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "\n",
    "        # TO COMPLETE\n",
    "        # remember no gradients are needed\n",
    "\n",
    "        # No gradients needed during forward pass\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "\n",
    "        # Flatten the output\n",
    "        features = features.view(features.size(0), -1)\n",
    "        \n",
    "        return features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ananconda\\envs\\torch\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "E:\\ananconda\\envs\\torch\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EncoderCNN(\n",
       "  (resnet): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate encoder and put into evaluation mode.\n",
    "# Your code here!\n",
    "\n",
    "# Instantiate the EncoderCNN\n",
    "cnnencoder = EncoderCNN().to(device)\n",
    "\n",
    "# Set the encoder to evaluation mode\n",
    "cnnencoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Image feature extraction step (7 marks)\n",
    "\n",
    "Pass the images through the ```Encoder``` model, saving the resulting features for each image. You may like to use a ```Dataset``` and ```DataLoader``` to load the data in batches for faster processing, or you may choose to simply read in one image at a time from disk without any loaders.\n",
    "\n",
    "Note that as this is a forward pass only, no gradients are needed. You will need to be able to match each image ID (the image name without file extension) with its features later, so we suggest either saving a dictionary of image ID: image features, or keeping a separate ordered list of image IDs.\n",
    "\n",
    "Use this ImageNet transform provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([ \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(224), \n",
    "    transforms.CenterCrop(224), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406),   # using ImageNet norms\n",
    "                         (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5068\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000000009.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000000000025.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000000000030.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000000000034.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000000000036.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5063</th>\n",
       "      <td>000000581906.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5064</th>\n",
       "      <td>000000581909.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5065</th>\n",
       "      <td>000000581913.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5066</th>\n",
       "      <td>000000581921.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5067</th>\n",
       "      <td>000000581929.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5068 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             file_name\n",
       "0     000000000009.jpg\n",
       "1     000000000025.jpg\n",
       "2     000000000030.jpg\n",
       "3     000000000034.jpg\n",
       "4     000000000036.jpg\n",
       "...                ...\n",
       "5063  000000581906.jpg\n",
       "5064  000000581909.jpg\n",
       "5065  000000581913.jpg\n",
       "5066  000000581921.jpg\n",
       "5067  000000581929.jpg\n",
       "\n",
       "[5068 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get unique images from the csv for extracting features - helper code\n",
    "imageList = pd.read_csv(\"./data/coco_subset_meta.csv\")\n",
    "imageList['file_name']\n",
    "len(imageList.id.unique())\n",
    "\n",
    "imagesUnique = sorted(imageList['file_name'].unique())\n",
    "print(len(imagesUnique))\n",
    "\n",
    "df_unique_files =  pd.DataFrame.from_dict(imagesUnique)\n",
    "\n",
    "df_unique_files.columns = ['file_name']\n",
    "df_unique_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class COCOImagesDataset(Dataset) function that takes the \n",
    "# image file names and reads the image and apply transform to it\n",
    "# ---> your code here! we have provided you a sketch \n",
    "\n",
    "# import package to load the image \n",
    "from PIL import Image\n",
    "\n",
    "IMAGE_DIR = \"./data/coco/images/\"\n",
    "\n",
    "class COCOImagesDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "\n",
    "        self.df = df\n",
    "        # --> your code here!\n",
    "        # create the transform\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filename = self.df.iloc[index]['file_name']\n",
    "\n",
    "        # --> your code here!\n",
    "        # create the image-path\n",
    "        image_path = os.path.join(IMAGE_DIR, filename)\n",
    "\n",
    "        # load the image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        # do the transform on the image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, filename\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Dataloader to use the unique files using the class COCOImagesDataset\n",
    "# make sure that shuffle is False as we are not aiming to retrain in this exercise\n",
    "# Your code here-->\n",
    "# import useful package\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Initialize the COCOImagesDataset\n",
    "dataset = COCOImagesDataset(df_unique_files, transform=data_transform)\n",
    "\n",
    "# Create a DataLoader with shuffle=False\n",
    "data_loader = DataLoader(dataset, batch_size=8, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm sorry Professor, I'm not sure why I can't run this section. Perhaps it's because my computer is quite old. Unfortunately, I'm also unable to run the rest of the code which need the feature map for a test check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply encoder to extract featues and save them (e.g., you can save it using image_ids)\n",
    "# Hint - make sure to save your features after running this - you can use torch.save to do this\n",
    "\n",
    "features_map = dict()\n",
    "from PIL import Image\n",
    "\n",
    "with torch.no_grad():\n",
    "# ---> Your code here!  \n",
    "# Loop through the DataLoader to get images and their IDs\n",
    "    for images, filenames in data_loader:\n",
    "        # Move images to the appropriate device (GPU or CPU)\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Pass images through the encoder to get features\n",
    "        features = cnnencoder(images)\n",
    "\n",
    "        # Store the features in the dictionary\n",
    "        for i, filename in enumerate(filenames):\n",
    "            image_id = os.path.splitext(filename)[0]\n",
    "            features_map[image_id] = features[i].cpu().numpy()\n",
    "\n",
    "# Save the features to a file\n",
    "torch.save(features_map, \"image_features.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfR--uYXHdIi"
   },
   "source": [
    "## 2 Text preparation [23 marks]\n",
    "\n",
    "> 2.1 Build the caption dataset (3 Marks)\n",
    "\n",
    "> 2.2 Clean the captions (3 marks)\n",
    "\n",
    "> 2.3 Split the data (3 marks)\n",
    "\n",
    "> 2.4 Building the vocabulary (10 marks)\n",
    "\n",
    "> 2.5 Prepare dataset using dataloader (4 marks)\n",
    "\n",
    "\n",
    "#### 2.1 Build the caption dataset (3 Marks)\n",
    "\n",
    "All our selected COCO_5029 images are from the official 2017 train set.\n",
    "\n",
    "The ```coco_subset_meta.csv``` file includes the image filenames and unique IDs of all the images in our subset. The ```id``` column corresponds to each unique image ID.\n",
    "\n",
    "The COCO dataset includes many different types of annotations: bounding boxes, keypoints, reference captions, and more. We are interested in the captioning labels. Open ```captions_train2017.json``` from the zip file downloaded from the COCO website. You are welcome to come up with your own way of doing it, but we recommend using the ```json``` package to initially inspect the data, then the ```pandas``` package to look at the annotations (if you read in the file as ```data```, then you can access the annotations dictionary as ```data['annotations']```).\n",
    "\n",
    "Use ```coco_subset_meta.csv``` to cross-reference with the annotations from ```captions_train2017.json``` to get all the reference captions for each image in COCO_5029.\n",
    "\n",
    "For example, you may end up with data looking like this (this is a ```pandas``` DataFrame, but it could also be several lists, or some other data structure/s):\n",
    "\n",
    "<img src=\"caption_image_ids.png\" alt=\"images matched to caption\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# loading captions for training\n",
    "with open('./data/coco/annotations2017/captions_train2017.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "df = pd.DataFrame.from_dict(data[\"annotations\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: get the filename matching id from coco_subset_meta.csv - make sure that for each id you add image filename\n",
    "coco_subset = pd.read_csv(\"./data/coco_subset_meta.csv\")\n",
    "# --> your code here! - name the new dataframe as \"new_file\"\n",
    "coco_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: get the filename matching id from coco_subset_meta.csv - make sure that for each id you add image filename\n",
    "coco_subset = pd.read_csv(\"./data/coco_subset_meta.csv\")\n",
    "# --> your code here! - name the new dataframe as \"new_file\"\n",
    "coco_subset.head()\n",
    "# Merge the annotations with coco_subset based on the image IDs\n",
    "new_file = pd.merge(coco_subset, df, on=['id'])\n",
    "\n",
    "# Drop unnecessary columns\n",
    "new_file = new_file.drop(columns=[\"Unnamed: 0\",\"license\", \"coco_url\", \"height\", \"width\", \"date_captured\", \"flickr_url\"])\n",
    "\n",
    "\n",
    "# Display the new DataFrame\n",
    "print(new_file.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Clean the captions (3 marks)\n",
    "\n",
    "Create a cleaned version of each caption. If using dataframes, we suggest saving the cleaned captions in a new column; otherwise, if you are storing your data in some other way, create data structures as needed. \n",
    "\n",
    "**A cleaned caption should be all lowercase, and consist of only alphabet characters.**\n",
    "\n",
    "Print out 10 original captions next to their cleaned versions to facilitate marking.\n",
    "\n",
    "\n",
    "<img src=\"cleancaptions.png\" alt=\"images matched to caption\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import useful packages\n",
    "import re\n",
    "\n",
    "new_file[\"clean_caption\"] = \"\" # add a new column to the dataframe for the cleaned captions\n",
    "\n",
    "def gen_clean_captions_df(df):\n",
    "\n",
    "    # Remove spaces in the beginning and at the end\n",
    "    # Convert to lower case\n",
    "    # Replace all non-alphabet characters with space\n",
    "    # Replace all continuous spaces with a single space\n",
    "\n",
    "    # -->your code here\n",
    "    clean_captions = []\n",
    "\n",
    "    for caption in df[\"caption\"]:\n",
    "        # Remove spaces in the beginning and at the end\n",
    "        cleaned = caption.strip()\n",
    "\n",
    "        # Convert to lower case\n",
    "        cleaned = cleaned.lower()\n",
    "\n",
    "        # Replace all non-alphabet characters with space\n",
    "        cleaned = re.sub(r'[^a-z]+', ' ', cleaned)\n",
    "\n",
    "        # Replace all continuous spaces with a single space\n",
    "        cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "\n",
    "        clean_captions.append(cleaned)\n",
    "\n",
    "    # Add to dataframe\n",
    "    df[\"clean_caption\"] = clean_captions\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and print 10 of these\n",
    "new_file = gen_clean_captions_df(new_file)\n",
    "new_file.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3  Split the data (3 marks)\n",
    "\n",
    "Split the data 70/10/20% into train/validation/test sets. **Be sure that each unique image (and all corresponding captions) only appear in a single set.**\n",
    "\n",
    "We provide the function below which, given a list of unique image IDs and a 3-split ratio, shuffles and returns  a split of the image IDs.\n",
    "\n",
    "If using a dataframe, ```df['image_id'].unique()``` will return the list of unique image IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "def split_ids(image_id_list, train=.7, valid=0.1, test=0.2):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image_id_list (int list): list of unique image ids\n",
    "        train (float): train split size (between 0 - 1)\n",
    "        valid (float): valid split size (between 0 - 1)\n",
    "        test (float): test split size (between 0 - 1)\n",
    "    \"\"\"\n",
    "    list_copy = image_id_list.copy()\n",
    "    random.shuffle(list_copy)\n",
    "    \n",
    "    train_size = math.floor(len(list_copy) * train)\n",
    "    valid_size = math.floor(len(list_copy) * valid)\n",
    "    \n",
    "    return list_copy[:train_size], list_copy[train_size:(train_size + valid_size)], list_copy[(train_size + valid_size):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the unique image IDs from the DataFrame\n",
    "unique_image_ids = new_file['image_id'].unique()\n",
    "\n",
    "# Split the image IDs into train, validation, and test sets\n",
    "train_ids, valid_ids, test_ids = split_ids(unique_image_ids, train=0.7, valid=0.1, test=0.2)\n",
    "\n",
    "# Create separate DataFrames for train, validation, and test sets based on the split IDs\n",
    "train_df = new_file[new_file['image_id'].isin(train_ids)]\n",
    "valid_df = new_file[new_file['image_id'].isin(valid_ids)]\n",
    "test_df = new_file[new_file['image_id'].isin(test_ids)]\n",
    "\n",
    "# Print the sizes of the DataFrames\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(valid_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Building the vocabulary (10 marks)\n",
    "\n",
    "The vocabulary consists of all the possible words which can be used - both as input into the model, and as output predictions, and we will build it using the cleaned words found in the reference captions from the training set. In the vocabulary each unique word is mapped to a unique integer (a Python ```dictionary``` object).\n",
    "\n",
    "A ```Vocabulary``` object is provided for you below to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\" Simple vocabulary wrapper which maps every unique word to an integer ID. \"\"\"\n",
    "    def __init__(self):\n",
    "        # intially, set both the IDs and words to dictionaries with special tokens\n",
    "        self.word2idx = {'<pad>': 0, '<unk>': 1, '<end>': 2}\n",
    "        self.idx2word = {0: '<pad>', 1: '<unk>', 2: '<end>'}\n",
    "        self.idx = 3\n",
    "\n",
    "    def add_word(self, word):\n",
    "        # if the word does not already exist in the dictionary, add it\n",
    "        if not word in self.word2idx:\n",
    "            # this will convert each word to index and index to word as you saw in the tutorials\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            # increment the ID for the next word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        # if we try to access a word not in the dictionary, return the id for <unk>\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all words from the cleaned captions in the **training and validation sets**, ignoring any words which appear 3 times or less; this should leave you with roughly 2200 words (plus or minus is fine). As the vocabulary size affects the embedding layer dimensions, it is better not to add the very infrequently used words to the vocabulary.\n",
    "\n",
    "Create an instance of the ```Vocabulary()``` object and add all your words to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Hint] building a vocab function such with frequent words e.g., setting MIN_FREQUENCY = 3\n",
    "from collections import Counter\n",
    "\n",
    "MIN_FREQUENCY = 3\n",
    "\n",
    "def build_vocab(df_ids, new_file):\n",
    "    \"\"\" \n",
    "    Parses training set token file captions and builds a Vocabulary object and dataframe for \n",
    "    the image and caption data\n",
    "\n",
    "    Returns:\n",
    "        vocab (Vocabulary): Vocabulary object containing all words appearing more than min_frequency\n",
    "    \"\"\"\n",
    "    word_mapping = Counter()\n",
    "\n",
    "    # Iterate over image IDs and get their clean captions\n",
    "    for index, id in enumerate(df_ids):\n",
    "        captions = list(new_file.loc[new_file['image_id']==id]['clean_caption'])\n",
    "\n",
    "        # Iterate over captions for each image ID\n",
    "        for caption in captions:\n",
    "            for word in caption.split():\n",
    "                # Update word count in the word_mapping dictionary\n",
    "                word_mapping[word] += 1\n",
    "\n",
    "    # Create a vocab instance\n",
    "    vocab = Vocabulary()\n",
    "\n",
    "    # Add the words to the vocabulary\n",
    "    for word, count in word_mapping.items():\n",
    "        # Ignore infrequent words to reduce the embedding size\n",
    "        if count >= MIN_FREQUENCY:\n",
    "            vocab.add_word(word)\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build your vocabulary for train, valid and test sets\n",
    "# ---> your code here!\n",
    "# Combine train and validation image IDs\n",
    "train_and_valid_ids = np.concatenate((train_ids, valid_ids))\n",
    "# Build the vocabulary using the combined train and validation set image IDs\n",
    "vocab = build_vocab(train_and_valid_ids, new_file)\n",
    "\n",
    "# Print the size of the vocabulary\n",
    "print(f\"Vocabulary size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Prepare dataset using dataloader (4 marks)\n",
    "\n",
    "Create a PyTorch ```Dataset``` class and a corresponding ```DataLoader``` for the inputs to the decoder. Create three sets: one each for training, validation, and test. Set ```shuffle=True``` for the training set DataLoader.\n",
    "\n",
    "The ```Dataset``` function ```__getitem__(self, index)``` should return three Tensors:\n",
    "\n",
    ">1. A Tensor of image features, dimension (1, 2048).\n",
    ">2. A Tensor of integer word ids representing the reference caption; use your ```Vocabulary``` object to convert each word in the caption to a word ID. Be sure to add the word ID for the ```<end>``` token at the end of each caption, then fill in the the rest of the caption with the ```<pad>``` token so that each caption has uniform lenth (max sequence length) of **47**.\n",
    ">3. A Tensor of integers representing the true lengths of every caption in the batch (include the ```<end>``` token in the count).\n",
    "\n",
    "\n",
    "Note that as each unique image has five or more (say, ```n```) reference captions, each image feature will appear ```n``` times, once in each unique (feature, caption) pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 47\n",
    "\n",
    "class COCO_Features(Dataset):\n",
    "    \"\"\" COCO subset custom dataset, compatible with torch.utils.data.DataLoader. \"\"\"\n",
    "    \n",
    "    def __init__(self, df, features, vocab):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: (dataframe or some other data structure/s you may prefer to use)\n",
    "            features: image features\n",
    "            vocab: vocabulary wrapper\n",
    "           \n",
    "        \"\"\"\n",
    "        \n",
    "        # TO COMPLETE\n",
    "        self.df = df\n",
    "        self.features = features\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Returns one data tuple (feature [1, 2048], target caption of word IDs [1, 47], and integer true caption length) \"\"\"   \n",
    "        # TO COMPLETE\n",
    "        \n",
    "        image_id = self.df.iloc[index]['image_id']\n",
    "        feature = self.features[image_id].unsqueeze(0)\n",
    "    \n",
    "        caption = self.df.iloc[index]['clean_caption'].split()\n",
    "        caption_length = len(caption) + 1  # Add 1 for the <end> token\n",
    "    \n",
    "        caption_ids = [self.vocab(word) for word in caption]\n",
    "        caption_ids.append(self.vocab('<end>'))\n",
    "    \n",
    "        # Pad the caption with <pad> tokens to reach MAX_SEQ_LEN\n",
    "        while len(caption_ids) < MAX_SEQ_LEN:\n",
    "            caption_ids.append(self.vocab('<pad>'))\n",
    "    \n",
    "        target_caption = torch.LongTensor(caption_ids).unsqueeze(0)\n",
    "        true_caption_length = torch.LongTensor([caption_length])\n",
    "    \n",
    "        return feature, target_caption, true_caption_length        \n",
    "       \n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caption_collate_fn(data):\n",
    "    \"\"\" Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "    Args:\n",
    "        data: list of tuple (image, caption). \n",
    "            - image: torch tensor of shape (3, 224, 224).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "    Returns:\n",
    "        images: torch tensor of shape (batch_size, 3, 224, 224).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    # Sort data list by caption length from longest to shortest.\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "\n",
    "    images, captions, lengths = zip(*data)\n",
    "\n",
    "    # Merge images (from tuple of 3D tensor to 4D tensor).\n",
    "    # If using features, merge 2D tensor to 3D tensor (batch_size, 2048).\n",
    "    images = torch.cat(images, 0)\n",
    "\n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap.squeeze(0)[:end]\n",
    "\n",
    "    return images, targets, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_workers = 4\n",
    "\n",
    "dataset_train = COCO_Features(\n",
    "    df=train_df,\n",
    "    vocab=vocab,\n",
    "    features=features_map,\n",
    ")\n",
    "\n",
    "#  your dataloader here (make shuffle true as you will be training RNN)\n",
    "# --> your code here!\n",
    "\n",
    "# DataLoader for the training set\n",
    "train_loader = DataLoader(\n",
    "    dataset=dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=caption_collate_fn\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Do the same as above for your validation set\n",
    "# ---> your code here!\n",
    "\n",
    "# Create the validation set dataset\n",
    "dataset_valid = COCO_Features(\n",
    "    df=valid_df,\n",
    "    vocab=vocab,\n",
    "    features=features_map,\n",
    ")\n",
    "\n",
    "# DataLoader for the validation set\n",
    "valid_loader = DataLoader(\n",
    "    dataset=dataset_valid,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=caption_collate_fn\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load one batch of the training set and print out the shape of each returned Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one batch from the train_loader\n",
    "dataiter = iter(train_loader)\n",
    "images, targets, lengths = dataiter.next()\n",
    "\n",
    "# Print out the shape of each returned Tensor\n",
    "print(f\"Images shape: {images.shape}\")\n",
    "print(f\"Targets shape: {targets.shape}\")\n",
    "print(f\"Lengths shape: {len(lengths)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Train DecoderRNN [20 marks]\n",
    "\n",
    "> 3.1 Design RNN-based decoder (10 marks)\n",
    "\n",
    "> 3.2 Train your model with precomputed features (10 Marks)\n",
    "\n",
    "#### 3.1 Design a RNN-based decoder (10 marks)\n",
    "\n",
    "Read through the ```DecoderRNN``` model below. First, complete the decoder by adding an ```RNN``` layer to the decoder where indicated, using [the PyTorch API as reference](https://pytorch.org/docs/stable/nn.html#rnn).\n",
    "\n",
    "Keep all the default parameters except for ```batch_first```, which you may set to True.\n",
    "\n",
    "In particular, understand the meaning of ```pack_padded_sequence()``` as used in ```forward()```. Refer to the [PyTorch ```pack_padded_sequence()``` documentation](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=256, hidden_size=512, num_layers=1, max_seq_length=47):\n",
    "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # we want a specific output size, which is the size of our embedding, so\n",
    "        # we feed our extracted features from the last convolutional layer (flattened to dimensions after AdaptiveAvgPool2d that may give you => 1 x 1 x 2048, other layers are also accepted but this will affect your accuracy!)\n",
    "        # into a Linear layer to resize\n",
    "        # your code\n",
    "        self.resize = nn.Linear(2048, embed_size)\n",
    "        \n",
    "        # batch normalisation helps to speed up training\n",
    "        # your code\n",
    "        self.bn = nn.BatchNorm1d(embed_size)\n",
    "\n",
    "        # your code for embedding layer\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        # your code for RNN\n",
    "        self.rnn = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        # self.linear: linear layer with input: hidden layer, output: vocab size\n",
    "        # --> your code\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "       \n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "\n",
    "    def forward(self, features, captions, lengths):\n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "        embeddings = self.embed(captions)\n",
    "        im_features = self.resize(features)\n",
    "        im_features = self.bn(im_features)\n",
    "        \n",
    "        # compute your feature embeddings\n",
    "        # your code\n",
    "        im_features = im_features.unsqueeze(1)\n",
    "        embeddings = torch.cat((im_features, embeddings), 1)\n",
    "    \n",
    "        # pack_padded_sequence returns a PackedSequence object, which contains two items: \n",
    "        # the packed data (data cut off at its true length and flattened into one list), and \n",
    "        # the batch_sizes, or the number of elements at each sequence step in the batch.\n",
    "        # For instance, given data [a, b, c] and [x] the PackedSequence would contain data \n",
    "        # [a, x, b, c] with batch_sizes=[2,1,1].\n",
    "\n",
    "        # your code [hint: use pack_padded_sequence]\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True)\n",
    "        hiddens, _ = self.rnn(packed)\n",
    "\n",
    "\n",
    "        outputs = self.linear(hiddens[0]) #hint: use a hidden layers in parenthesis\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "    def sample(self, features, states=None):\n",
    "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
    "        sampled_ids = []\n",
    "\n",
    "        inputs = self.bn(self.resize(features)).unsqueeze(1)\n",
    "        for i in range(self.max_seq_length):\n",
    "            hiddens, states = self.rnn(inputs, states)  # hiddens: (batch_size, 1, hidden_size)\n",
    "            outputs = self.linear(hiddens.squeeze(1))   # outputs:  (batch_size, vocab_size)\n",
    "            _, predicted = outputs.max(1)               # predicted: (batch_size)\n",
    "            sampled_ids.append(predicted)\n",
    "            inputs = self.embed(predicted)              # inputs: (batch_size, embed_size)\n",
    "            inputs = inputs.unsqueeze(1)                # inputs: (batch_size, 1, embed_size)\n",
    "        sampled_ids = torch.stack(sampled_ids, 1)       # sampled_ids: (batch_size, max_seq_length)\n",
    "        return sampled_ids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate decoder\n",
    "# your code here!\n",
    "vocab_size = len(vocab)  # Vocabulary size\n",
    "embed_size = 256  # Embedding size\n",
    "hidden_size = 512  # Hidden size\n",
    "num_layers = 1  # Number of layers for the LSTM\n",
    "\n",
    "# Instantiate the decoder\n",
    "rnndecoder = DecoderRNN(vocab_size, embed_size, hidden_size, num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Train your model with precomputed features (10 marks)\n",
    "\n",
    "Train the decoder by passing the features, reference captions, and targets to the decoder, then computing loss based on the outputs and the targets. Note that when passing the targets and model outputs to the loss function, the targets will also need to be formatted using ```pack_padded_sequence()```.\n",
    "\n",
    "We recommend a batch size of around 64 (though feel free to adjust as necessary for your hardware).\n",
    "\n",
    "**We strongly recommend saving a checkpoint of your trained model after training so you don't need to re-train multiple times.**\n",
    "\n",
    "Display a graph of training and validation loss over epochs to justify your stopping point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnndecoder.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training and validation losses\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Training loop\n",
    "    rnndecoder.train()\n",
    "    train_loss = 0.0\n",
    "    for i, (features, captions, lengths) in enumerate(train_loader):\n",
    "        # Move data to device\n",
    "        features = features.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = rnndecoder(features, captions, lengths)\n",
    "\n",
    "        # Calculate loss\n",
    "        targets = pack_padded_sequence(captions, lengths, batch_first=True).data\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation loop\n",
    "    decoder.eval()\n",
    "    valid_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (features, captions, lengths) in enumerate(valid_loader):\n",
    "            # Move data to device\n",
    "            features = features.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = rnndecoder(features, captions, lengths)\n",
    "\n",
    "            # Calculate loss\n",
    "            targets = pack_padded_sequence(captions, lengths, batch_first=True).data\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    valid_loss /= len(valid_loader)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(decoder.state_dict(), \"decoder.pth\")\n",
    "\n",
    "# Plot training and validation losses\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(valid_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXzlLbstCE7f"
   },
   "source": [
    "## 4 Generate predictions on test data [8 marks]\n",
    "\n",
    "Display 5 sample test images containing different objects, along with your model’s generated captions and all the reference captions for each.\n",
    "\n",
    "> Remember that everything **displayed** in the submitted notebook and .html file will be marked, so be sure to run all relevant cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image_path, image_id, vocab):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Extract features using pretrained CNN\n",
    "    features = encoder(image)\n",
    "    \n",
    "    # Generate caption using the trained decoder\n",
    "    sampled_ids = decoder.sample(features)\n",
    "    sampled_ids = sampled_ids[0].cpu().numpy()\n",
    "    \n",
    "    # Convert word IDs to words\n",
    "    caption = []\n",
    "    for word_id in sampled_ids:\n",
    "        word = vocab.idx2word[word_id]\n",
    "        caption.append(word)\n",
    "        if word == '<end>':\n",
    "            break\n",
    "    caption = ' '.join(caption)\n",
    "    \n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "test_dataset = COCO_Features(test_set, features_map, vocab)\n",
    "\n",
    "# Display 5 sample test images and generated captions\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(len(test_set), 5)\n",
    "for index in sample_indices:\n",
    "    image_id = test_set.iloc[index]['image_id']\n",
    "    image_path = os.path.join(image_folder, test_set.iloc[index]['file_name'])\n",
    "    \n",
    "    # Generate caption\n",
    "    generated_caption = generate_caption = generate_caption(image_path, image_id, vocab)\n",
    "    print(\"\\n\" + \"-\" * 80 + \"\\n\")\n",
    "    \n",
    "    # Display the image and generated caption\n",
    "    plt.imshow(np.array(Image.open(image_path).convert(\"RGB\")))\n",
    "    plt.title(f\"Generated caption: {caption}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Print reference captions\n",
    "    ref_captions = list(test_set.loc[test_set['image_id'] == image_id]['caption'])\n",
    "    print(\"Reference captions:\")\n",
    "    for ref_caption in ref_captions:\n",
    "        print(ref_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bVfEgbC4I_dE"
   },
   "source": [
    "## 5 Caption evaluation using BLEU score [10 marks]\n",
    "\n",
    "There are different methods for measuring the performance of image to text models. We will evaluate our model by measuring the text similarity between the generated caption and the reference captions, using two commonly used methods. Ther first method is known as *Bilingual Evaluation Understudy (BLEU)*.\n",
    "\n",
    "> 5.1 Average BLEU score on all data (5 marks)\n",
    "\n",
    "> 5.2 Examplaire high and low score BLEU score samples (5 marks, at least two)\n",
    "\n",
    "####  5.1 Average BLEU score on all data (5 marks)\n",
    "\n",
    "\n",
    "One common way of comparing a generated text to a reference text is using BLEU. This article gives a good intuition to how the BLEU score is computed: https://machinelearningmastery.com/calculate-bleu-score-for-text-python/, and you may find an implementation online to use. One option is the NLTK implementation `nltk.translate.bleu_score` here: https://www.nltk.org/api/nltk.translate.bleu_score.html\n",
    "\n",
    "\n",
    "> **Tip:** BLEU scores can be weighted by ith-gram. Check that your scores make sense; and feel free to use a weighting that best matches the data. We will not be looking for specific score ranges; rather we will check that the scores are reasonable and meaningful given the captions.\n",
    "\n",
    "Write the code to evaluate the trained model on the complete test set and calculate the BLEU score using the predictions, compared against all five references captions. \n",
    "\n",
    "Display a histogram of the distribution of scores over the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xypfUN7y4CKI"
   },
   "outputs": [],
   "source": [
    "# TO COMPLETE\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "stats = pd.DataFrame(columns=['ref','preds','bleu','cos_sim'])#dict()\n",
    "\n",
    "# --> Your code here!\n",
    "bleu_scores = []\n",
    "for index in range(len(test_set)):\n",
    "    image_id = test_set.iloc[index]['image_id']\n",
    "    image_path = os.path.join(image_folder, test_set.iloc[index]['file_name'])\n",
    "    \n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Generate caption\n",
    "    generated_caption = generate_caption = generate_caption(image_path, image_id, vocab)\n",
    "    \n",
    "    # Get reference captions\n",
    "    reference_captions = list(test_set.loc[test_set['image_id'] == image_id]['caption'])\n",
    "    \n",
    "    # Calculate BLEU score\n",
    "    bleu_score = sentence_bleu(reference_captions, generated_caption)\n",
    "    bleu_scores.append(bleu_score)\n",
    "    \n",
    "    # Add to stats DataFrame\n",
    "    stats = stats.append({\"ref\": reference_captions, \"preds\": generated_caption, \"bleu\": bleu_score}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average BLEU score:\", stats['bleu'].mean())\n",
    "ax = stats['bleu'].plot.hist(bins=100, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Examplaire high and low score BLEU score samples (5 marks)\n",
    "\n",
    "Find one sample with high BLEU score and one with a low score, and display the model's predicted sentences, the BLEU scores, and the 5 reference captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO COMPLETE\n",
    "sorted_stats = stats.sort_values(by=['bleu'], ascending=False)\n",
    "\n",
    "high_score_sample = sorted_stats.iloc[0]\n",
    "low_score_sample = sorted_stats.iloc[-1]\n",
    "\n",
    "def display_image_and_captions(sample, title):\n",
    "    image_id = test_set.loc[test_set['caption'] == sample['ref'][0]]['image_id'].values[0]\n",
    "    image_path = os.path.join(image_folder, test_set.loc[test_set['image_id'] == image_id]['file_name'].values[0])\n",
    "    \n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    plt.imshow(image)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Predicted caption: {sample['preds']}\")\n",
    "    print(f\"BLEU score: {sample['bleu']}\")\n",
    "    print(\"Reference captions:\")\n",
    "    for ref in sample['ref']:\n",
    "        print(f\"- {ref}\")\n",
    "\n",
    "display_image_and_captions(high_score_sample, \"High BLEU score sample\")\n",
    "print(\"\\n\")\n",
    "display_image_and_captions(low_score_sample, \"Low BLEU score sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VQKNo384CKP"
   },
   "source": [
    "## 6 Caption evaluation using cosine similarity [12 marks]\n",
    "\n",
    "> 6.1 Cosine similarity (6 marks)\n",
    "\n",
    "> 6.2 Cosine similarity examples (6 marks)\n",
    "\n",
    "####  6.1 Cosine similarity (6 marks)\n",
    "\n",
    "The cosine similarity measures the cosine of the angle between two vectors in n-dimensional space. The smaller the angle, the greater the similarity.\n",
    "\n",
    "To use the cosine similarity to measure the similarity between the generated caption and the reference captions: \n",
    "\n",
    "* Find the embedding vector of each word in the caption \n",
    "* Compute the average vector for each caption \n",
    "* Compute the cosine similarity score between the average vector of the generated caption and average vector of each reference caption\n",
    "* Compute the average of these scores \n",
    "\n",
    "Calculate the cosine similarity using the model's predictions over the whole test set. \n",
    "\n",
    "Display a histogram of the distribution of scores over the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO COMPLETE\n",
    "def average_embedding(caption, embedding_layer):\n",
    "    words = caption.split()\n",
    "    word_embeddings = [embedding_layer(torch.tensor(vocab[word])).numpy() for word in words]\n",
    "    return np.mean(word_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sim_scores = []\n",
    "for index, row in stats.iterrows():\n",
    "    pred_embedding = average_embedding(row['preds'], decoder.embed)\n",
    "    ref_embeddings = [average_embedding(ref, decoder.embed) for ref in row['ref']]\n",
    "    similarities = [cosine_similarity([pred_embedding], [ref_emb]) for ref_emb in ref_embeddings]\n",
    "    avg_similarity = np.mean(similarities)\n",
    "    cos_sim_scores.append(avg_similarity)\n",
    "\n",
    "stats['cos_sim'] = cos_sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(stats['cos_sim'], bins=20)\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Cosine Similarity Scores over the Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Cosine similarity examples (6 marks)\n",
    "\n",
    "Find one sample with high cosine similarity score and one with a low score, and display the model's predicted sentences, the cosine similarity scores, and the 5 reference captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO COMPLETE\n",
    "sorted_stats = stats.sort_values(by=['cos_sim'], ascending=False).reset_index(drop=True)\n",
    "high_cos_sim_example = sorted_stats.iloc[0]\n",
    "low_cos_sim_example = sorted_stats.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cos_sim_example(example, title):\n",
    "    print(title)\n",
    "    print('-' * len(title))\n",
    "    print(\"Generated caption:\", example['preds'])\n",
    "    print(\"Cosine similarity score:\", example['cos_sim'])\n",
    "    print(\"Reference captions:\")\n",
    "    for i, ref in enumerate(example['ref']):\n",
    "        print(f\"{i + 1}. {ref}\")\n",
    "    print()\n",
    "\n",
    "display_cos_sim_example(high_cos_sim_example, \"High Cosine Similarity Example\")\n",
    "display_cos_sim_example(low_cos_sim_example, \"Low Cosine Similarity Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDmwrp-w4CKR"
   },
   "source": [
    "## 7 Comparing BLEU and Cosine similarity [16 marks]\n",
    "\n",
    "> 7.1 Test set distribution of scores (6 marks)\n",
    "\n",
    "> 7.2 Analysis of individual examples (10 marks)\n",
    "\n",
    "#### 7.1 Test set distribution of scores (6 marks)\n",
    "\n",
    "Compare the model’s performance on the test set evaluated using BLEU and cosine similarity and discuss some weaknesses and strengths of each method (explain in words, in a text box below). \n",
    "\n",
    "Please note, to compare the average test scores, you need to rescale the Cosine similarity scores [-1 to 1] to match the range of BLEU method [0.0 - 1.0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y2O8TZG74CKS"
   },
   "outputs": [],
   "source": [
    "# TO COMPLETE\n",
    "# Calculate the average BLEU and cosine similarity scores\n",
    "avg_bleu = stats['bleu'].mean()\n",
    "avg_cos_sim = stats['cos_sim'].mean()\n",
    "\n",
    "# Rescale cosine similarity scores to [0.0, 1.0]\n",
    "rescaled_cos_sim = (avg_cos_sim + 1) / 2\n",
    "\n",
    "print(\"Average BLEU score:\", avg_bleu)\n",
    "print(\"Rescaled average cosine similarity score:\", rescaled_cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qigb0A9F4CKV"
   },
   "source": [
    "Strengths and weaknesses of BLEU:\n",
    "\n",
    "Strengths:\n",
    "BLEU score considers word order and n-grams, which means it takes into account the local context of words.\n",
    "The score is easy to interpret, with 0.0 indicating no match and 1.0 indicating a perfect match.\n",
    "Weaknesses:\n",
    "BLEU can be sensitive to small changes in the text, and slight rephrasings might result in a lower score even if the meaning is preserved.\n",
    "It does not take into account the semantic similarity of words or phrases, so semantically similar but syntactically different sentences might get low scores.\n",
    "Strengths and weaknesses of cosine similarity:\n",
    "\n",
    "Strengths:\n",
    "Cosine similarity captures the semantic similarity between sentences, as it's based on word embeddings.\n",
    "It is less sensitive to changes in word order or phrasing, making it more robust to paraphrasing.\n",
    "Weaknesses:\n",
    "It does not consider word order or context, which can be important for understanding the meaning of a sentence.\n",
    "The score ranges from -1 to 1, making it less intuitive than BLEU scores, which range from 0 to 1.\n",
    "\n",
    "In conclusion, both BLEU and cosine similarity have their strengths and weaknesses in evaluating generated captions. BLEU is more suitable for evaluating syntactic correctness, while cosine similarity is more appropriate for evaluating semantic similarity. Using both methods together can give a more comprehensive evaluation of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 7.2 Analysis of individual examples (10 marks)\n",
    " \n",
    "Find and display one example where both methods give similar scores and another example where they do not and discuss. Include both scores, predicted captions, and reference captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO COMPLETE\n",
    "# Calculate the differences between the rescaled cosine similarity scores and BLEU scores\n",
    "stats['score_diff'] = abs(stats['bleu'] - (stats['cos_sim'] + 1) / 2)\n",
    "\n",
    "# Find the indices of the minimum and maximum differences\n",
    "min_diff_index = stats['score_diff'].idxmin()\n",
    "max_diff_index = stats['score_diff'].idxmax()\n",
    "\n",
    "# Get the examples\n",
    "similar_example = stats.loc[min_diff_index]\n",
    "dissimilar_example = stats.loc[max_diff_index]\n",
    "\n",
    "def display_example(example):\n",
    "    print(\"Predicted caption:\", example['preds'])\n",
    "    print(\"Reference captions:\", example['ref'])\n",
    "    print(\"BLEU score:\", example['bleu'])\n",
    "    print(\"Cosine similarity score:\", example['cos_sim'])\n",
    "    print(\"Rescaled cosine similarity score:\", (example['cos_sim'] + 1) / 2)\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"Example with similar scores:\")\n",
    "display_example(similar_example)\n",
    "\n",
    "print(\"Example with dissimilar scores:\")\n",
    "display_example(dissimilar_example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example with similar scores:\n",
    "In this example, both the BLEU score and the cosine similarity score indicate that the predicted caption is similar to the reference captions. This suggests that the predicted caption captures both the syntactic correctness and semantic similarity of the reference captions.\n",
    "\n",
    "Example with dissimilar scores:\n",
    "In this example, the BLEU score and cosine similarity score disagree on the quality of the predicted caption. This can occur for a variety of reasons, such as:\n",
    "\n",
    "The predicted caption may have a different word order or phrasing compared to the reference captions, resulting in a lower BLEU score but a higher cosine similarity score.\n",
    "The predicted caption may use synonyms or semantically similar words that are not present in the reference captions, resulting in a lower BLEU score but a higher cosine similarity score.\n",
    "The predicted caption may have a similar word order and phrasing compared to the reference captions, but it may lack semantic similarity, resulting in a higher BLEU score but a lower cosine similarity score.\n",
    "In such cases, it is important to consider both scores and evaluate the generated captions from both syntactic and semantic perspectives to get a comprehensive understanding of the model's performance"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "COMP5623M_CW1_Q2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
