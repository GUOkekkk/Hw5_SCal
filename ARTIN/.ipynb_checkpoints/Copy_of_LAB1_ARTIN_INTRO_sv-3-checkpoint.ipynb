{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQB6jUVevZRb"
   },
   "source": [
    "# ARTificial INtelligence ARTIN\n",
    "**Master CORO - Centrale Nantes**\n",
    "\n",
    "**Autor: Diana Mateus**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alKP72LX0DnN"
   },
   "source": [
    "# Introductory TD1\n",
    "\n",
    "Scikit-learn is a very popular Machine Learning library for Python. In this notebook we will study how to put in practice the the three simple machine learning models \n",
    "\n",
    "*   Linear Regression\n",
    "*   K-nearest neighbors\n",
    "*   Naive Bayes \n",
    "\n",
    "seen during the lecture relying on scikit-learn.\n",
    "\n",
    "**GOALS**: \n",
    "\n",
    "*   Splitting data for Machine Learning\n",
    "*   Discovering the simple three step mechanism to load, train and test an existing model from the scikit library\n",
    "\n",
    "\n",
    "**DELIVERABLES**:\n",
    "There are is no code to deliver. However, while running the code, you should make sure you understand:\n",
    "\n",
    "*   What is the purpose of data splitting.\n",
    "*   What are the different lines in the code doing. Especially what is happening each time you call ``fit``.\n",
    "*   And to answering to the questions at the end of the notebook, for which you are invited to modify the code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7l9znFXqw3h"
   },
   "source": [
    "# Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vy_xMsgDq0jx"
   },
   "outputs": [],
   "source": [
    "import numpy as np #scientific computing (in ML it handles and operates on multi-dimensional arrays)\n",
    "import matplotlib.pyplot as plt #for data visualization\n",
    "import sklearn #for Machine Learning\n",
    "import pandas as pd #for reading, writing and processing databases \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GeEfQr-WraKX"
   },
   "source": [
    "# Loading and Splitting Data\n",
    "\n",
    "The objective of a Supervised Machine Learning methods is\n",
    "\n",
    "*   to learn from examples\n",
    "*   to be able to make predictions\n",
    "*   for unseen data!!!\n",
    "\n",
    "The primary rule when training a model is therefore to split all the available  data with supervised data in groups:\n",
    "*  **Training set** : used to fit the model parameters.\n",
    "*  **Validation set** : used to set the model hyper-parameters.\n",
    "*  **Test set** : used only after training and validation have been finished to evaluate the performance of the method.\n",
    "\n",
    "For real life problems is important to reduce the use of the test set to its minimum, to improve generalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zyK0InZ2N3b"
   },
   "source": [
    "### Datasets for Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRYBPKoXtRNg"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets \n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "\n",
    "\n",
    "# Explore on your own the dimensions of the dataset and their meaning? w\n",
    "X = diabetes.data[:, np.newaxis, 2] \n",
    "\n",
    "# SPLIT what is the effect of the following lines?\n",
    "X_train = X[:-30]\n",
    "X_test = X[-30:]\n",
    "y_train = diabetes.target[:-30]\n",
    "y_test = diabetes.target[-30:]\n",
    "\n",
    "# Explore the data\n",
    "# print(diabetes.DESCR) #Uncomment to see the dataset description\n",
    "print(diabetes.feature_names)\n",
    "print(diabetes.target.shape)\n",
    "print('X train', X_train.shape)\n",
    "print('X test',X_test.shape)\n",
    "print('y train',y_train.shape)\n",
    "print('y test',y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBlpHylU7cbZ"
   },
   "source": [
    "### Datasets for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwPqKKx8rZ2U"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "\n",
    "label_names = data['target_names']\n",
    "labels = data['target']\n",
    "feature_names = data['feature_names']\n",
    "features = data['data']\n",
    "\n",
    "print(label_names)\n",
    "print(labels.shape)\n",
    "print(feature_names)\n",
    "print(features.shape)\n",
    "#print(data.DESCR) #Uncomment to see the dataset description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_nRhb-PHruH9"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(features,labels,test_size = 0.40, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8bb5p0PqVv0"
   },
   "source": [
    "# Training a ML model\n",
    "\n",
    "When relying with on the scikit library, training a model is very simple. You  need to:\n",
    "*   Load the model from scikit\n",
    "*   Declare a new instance of the model \n",
    "*   Fit the parameters\n",
    "*   Make predictions for new data\n",
    "\n",
    "Identify in the example code the above steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQZxSTfNAY5T"
   },
   "source": [
    "\n",
    "### Model 1. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tqUdMUhttNL0"
   },
   "outputs": [],
   "source": [
    "#Load and declare a new instance\n",
    "from sklearn import linear_model\n",
    "regr = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyaY_YzU9oUY"
   },
   "outputs": [],
   "source": [
    "#Fit (train) the model \n",
    "regr.fit(X_train, y_train)\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "print('Intercept: \\n', regr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Psq8bTAL9oCq"
   },
   "outputs": [],
   "source": [
    "#Make predictions\n",
    "y_pred = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8bJhCgqX__rk"
   },
   "outputs": [],
   "source": [
    "#Evaluate the performance\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))\n",
    "print('Variance score: %.2f' % r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_jgPq4a-9po-"
   },
   "outputs": [],
   "source": [
    "#Visualize \n",
    "plt.scatter(X_test, y_test, color='blue')\n",
    "plt.plot(X_test, y_pred, color='red', linewidth=3)\n",
    "#plt.xticks(())\n",
    "#plt.yticks(())\n",
    "plt.xlabel('bmi')\n",
    "plt.ylabel('diabetes progression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMLpS4k1qZC0"
   },
   "source": [
    "## Model 2. K-Nearest Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45Ma2BaVsqIF"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knnClassifier = KNeighborsClassifier(n_neighbors=2)\n",
    "knnClassifier.fit(X2_train, y2_train)\n",
    "y2_pred_knn = knnClassifier.predict(X2_test)\n",
    "print(y2_pred_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvDv8enObTc2"
   },
   "outputs": [],
   "source": [
    "#Compute accuracy on the training set\n",
    "train_accuracy = knnClassifier.score(X2_train, y2_train)\n",
    "    \n",
    "#Compute accuracy on the test set\n",
    "test_accuracy = knnClassifier.score(X2_test, y2_test) \n",
    "\n",
    "print('train accuracy', train_accuracy)\n",
    "print('test accuracy',test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Uy3nUkjqdNl"
   },
   "source": [
    "## Model 3. Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEkGgcgUqn_K"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnbClassifier = GaussianNB()\n",
    "gnbClassifier.fit(X2_train,y2_train)\n",
    "y2_pred_gnb = gnbClassifier.predict(X2_test)\n",
    "print(y2_pred_gnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a1uU7olRcdpM"
   },
   "outputs": [],
   "source": [
    "#Compute accuracy on the training set\n",
    "train_accuracy = knnClassifier.score(X2_train, y2_train)\n",
    "    \n",
    "#Compute accuracy on the test set\n",
    "test_accuracy = knnClassifier.score(X2_test, y2_test) \n",
    "\n",
    "print('train accuracy', train_accuracy)\n",
    "print('test accuracy',test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iftgXOiztm1y"
   },
   "source": [
    "# QUESTIONS\n",
    "\n",
    "\n",
    "1.   Why is it wrong to use all available data for training (no data split)?\n",
    "2.   What is the effect of increasing the amount of training data? (In the regression example, try different amounts and plot the train and test errors for each case)\n",
    "3.   Only one feature 'bmi' was used during training, is it using more information better? (try adding new features).\n",
    "4.   How do we know if the learning was succesful in each case?\n",
    "5.   What model for classification is better?, why?\n",
    "6.   For which of the above models is it useful to split the data in three groups (instead of two) to also consider a validation set?\n",
    "7.   What is the best neighborhood size for the knn classifier, what is the appropriate methodologogy to find this number? \n",
    "8.   Naive classifiers are probabilistic classifiers. How do we recover the probabilistic information associated to this model? What quantities can we recover\n",
    "9.   We saw in the lecture that linear models were used for regression, while KNN and Naive Bayes were used for classification. Can we use linear models for classification?  KNN or Naive Bayes for regression problems?\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of LAB1-ARTIN-INTRO-sv.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
