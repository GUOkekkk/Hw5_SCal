{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks \n",
    "\n",
    "\n",
    "** Ecole Centrale Nantes **\n",
    "\n",
    "** Diana Mateus **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Participants : **\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General description\n",
    "In this lab we will create a simple classifier based on neural networks. We will progress in two parts:\n",
    "- In the first part, and to better understand the involved operations, we will create a single-neuron model and optimize its parameters \"by hand\". For this first part we will only use the **Numpy** library\n",
    "- We will then build a multi-layer perceptron with the built-in library **Keras** module and **tensorflow**. For this second part you should use **collab** (online platform) or have tensorflow installed in your computer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset\n",
    "Start by runing the following lines to load and visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    train_dataset = h5py.File('dataset/train_catvnoncat.h5', \"r\")\n",
    "    train_x = np.array(train_dataset[\"train_set_x\"][:]) \n",
    "    train_y = np.array(train_dataset[\"train_set_y\"][:])\n",
    "    test_dataset = h5py.File('dataset/test_catvnoncat.h5', \"r\")\n",
    "    test_x = np.array(test_dataset[\"test_set_x\"][:]) \n",
    "    test_y = np.array(test_dataset[\"test_set_y\"][:])\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) \n",
    "    \n",
    "    train_y = train_y.reshape((1, train_y.shape[0]))\n",
    "    test_y = test_y.reshape((1, test_y.shape[0]))\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y, classes\n",
    "\n",
    "train_x, train_y, test_x, test_y, classes=load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run several times to visualize different data points\n",
    "# the title shows the ground truth class labels (0=no cat , 1 = cat)\n",
    "index = np.random.randint(low=0,high=train_y.shape[1])\n",
    "plt.imshow(train_x[index])\n",
    "plt.title(str(train_y[0,index]))\n",
    "plt.show()\n",
    "print (\"Train X shape: \" + str(train_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing\n",
    "In the following lines we vectorize the images (Instead of a 2-D image we will give as input to the models a 1-D vector). The normalization makes the image intensity be between 0 and 1, and converts the images to floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.reshape(train_x.shape[0], -1).T\n",
    "test_x = test_x.reshape(test_x.shape[0], -1).T\n",
    "print (\"Train X shape: \" + str(train_x.shape))\n",
    "print (\"Train Y shape: \" + str(train_y.shape))\n",
    "print (\"Test X shape: \" + str(test_x.shape))\n",
    "print (\"Test Y shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x/255.\n",
    "test_x = test_x/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Classification with a single neuron \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Fill-in the follwoign three functions to define the single neuron model:\n",
    "- A function **initialize_parameters** that randomly initializes the weights with small and bias according to the dimension of the input (size of the image) given as parameter to the function\n",
    "- A function **sigmoid** that computes the sigmoid activation function\n",
    "- A function **neuron** that given an input vector, weights and biases computes the output of the single neuron model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(dim):\n",
    "    w = \n",
    "    b = \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron(w,b,X):\n",
    "    pred_y = \n",
    "    return pred_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** **Forward Pass:**\n",
    "Use the three functions above to compute a first forward pass for the input matrix $X$ containing the loaded dataset, for some initialization of the weights and bias.\n",
    " \n",
    " \\begin{align}\n",
    " Y_{\\rm pred}=\\sigma(w^\\top X+b) = [y_{\\rm pred}^{(1)},y_{\\rm pred}^{(2)},\\dots,y_{\\rm pred}^{(m)}]\n",
    " \\end{align}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) Cost estimation:**\n",
    " \n",
    "We will use a cross-entropy loss, so that the empirical risk can be computed as:\n",
    " \\begin{align}\n",
    " E = - \\frac{1}{m} \\sum_{i=1}^m \n",
    " y^{(i)} \\log(y_{\\rm pred}^{(i)}) +\n",
    " (1-y^{(i)}) \\log(1-y_{\\rm pred}^{(i)})\n",
    " \\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentropy(Y,Ypred):\n",
    "    \n",
    "    cost =   \n",
    "    \n",
    "    return cost\n",
    "\n",
    "cost(train_y,pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d) Back propagation:**\n",
    "\n",
    "After initializing the parameters and doing a forward pass, we need to backpropagate the cost by computing the gradient with respect to the model parameters to later update the weights\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial w} = & \\frac{1}{m} X(Y_{\\rm pred}-Y)^T\\\\\n",
    "\\frac{\\partial E}{\\partial b} = & \\frac{1}{m} (Y_{\\rm pred}-Y)\\\\\n",
    "\\end{align}\n",
    "\n",
    "See a demonstration in the wikipedia page \n",
    "https://en.wikipedia.org/wiki/Cross_entropy\n",
    "\n",
    "Fill-in the backpropagation function which receives as input the the training set (X,Y), as well as the current predictions and returns the gradients for the weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate(X, Y, Ypred):\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    #find gradient (back propagation)\n",
    "    dw = \n",
    "    db = \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db} \n",
    "    \n",
    "    return grads\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e) Optimization**\n",
    "After initializing the parameters, computing the cost function, and calculating gradients, we can now update the parameters using gradient descent. Use the functions implemented above to fill_in the \"gradient_descent\" function that optimizes the parameters given a training set X, Y, a fixed number of iterations, and a learning_rate. Store and plot the value of the loss function at each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, iterations, learning_rate):\n",
    "    costs = []\n",
    "    w, b = initialize_parameters(train_x.shape[0])\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        Ypred = \n",
    "        cost = \n",
    "        grads= \n",
    "        \n",
    "        #update parameters\n",
    "        w = \n",
    "        b = \n",
    "        costs.append(cost)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "       \n",
    "    return w,b, costs\n",
    "\n",
    "w, b, costs = gradient_descent(train_x,train_y,iterations=2000, learning_rate = 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e) Plot the training curve**\n",
    "Plot the evolution of the cost vs the iterations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f) Prediction**\n",
    "Use the optimized parameters to make predictions both for the train and test sets and compute the accuracy for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):    \n",
    "\n",
    "    return y_pred\n",
    "\n",
    "# predict \n",
    "train_pred_y = predict(w, b, train_x)\n",
    "test_pred_y = predict(w, b, test_x)\n",
    "print(\"Train Acc: {} %\".format(100 - np.mean(np.abs(train_pred_y - train_y)) * 100))\n",
    "print(\"Test Acc: {} %\".format(100 - np.mean(np.abs(test_pred_y - test_y)) * 100))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. CNNs with Keras\n",
    "\n",
    "Adapt the example in this website https://keras.io/examples/vision/mnist_convnet/ to our problem. To this end:\n",
    "- change the number of classes and the input size\n",
    "- it is not necessary to expand the dimensions since our input is 3dimensional expand_dims(x_train, -1)\n",
    "- you may need to transopose the labels vector\n",
    "- change the categorical cross-entropy to the binary cross entropy given that our problem is binary classification. \n",
    "- also change the softmax to sigmoid, the more appropriate activation function for binary data\n",
    "\n",
    "We can choose a single neuron output passed through sigmoid, and then set a threshold to choose the class, or use two neuron output and then perform a softmax.\n",
    "\n",
    "Can you get the accuracy better than in our hand made perceptron?\n",
    "\n",
    "Make a report about your adaptions to improve the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, split between train and test sets\n",
    "x_train, y_train, x_test, y_test, classes=load_dataset()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the model\n",
    "model =\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comiple and fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
